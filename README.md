Selamlar ğŸŒ·

* **benim bir baÅŸka serÃ¼venim ile karÅŸÄ± karÅŸÄ±yasÄ±nÄ±z bu benim ilk stajÄ±m 04.07.2025 - 30.07.2025 tarihlerinde staj iÃ§in yaptÄ±ÄŸÄ±m her ÅŸeyi dokÃ¼mante ediyor olacaÄŸÄ±m...**
* **Apple Silicon Mac kullandÄ±ÄŸÄ±mÄ± bilginize sunmak isterim Ã§Ã¼nkÃ¼ bazÄ± kurulum ve adÄ±mlar farklÄ± olabilir.**
  

  
## ğŸ“ Ã–n Kabul GÃ¶revleri

1. **GeliÅŸtirme OrtamÄ± Kurulumu (VirtualBox + Debian ARM64)**

   * **AdÄ±m 1:** VirtualBox indirme ve kurulum

     ```bash
     # https://www.virtualbox.org adresinden macOS sÃ¼rÃ¼mÃ¼nÃ¼ indirip kurun.
     ```

     > **AÃ§Ä±klama:** VirtualBox, Apple Silicon destekli Ã¶n sÃ¼rÃ¼mleriyle ARM64 tabanlÄ± VM oluÅŸturmanÄ±zÄ± saÄŸlar.
   * **AdÄ±m 2:** Debian ARM64 ISO hazÄ±rlama

     ```bash
     # Debian ARM64 ISO dosyasÄ±nÄ± https://www.debian.org/ARM64 adresinden indirin.
     ```

     > **AÃ§Ä±klama:** ARM64 daÄŸÄ±tÄ±mÄ±, M3 Ã§iple uyumlu Ã§alÄ±ÅŸÄ±r.
   * **AdÄ±m 3:** Yeni VM oluÅŸturma ve ayarlar

     | Ã–ÄŸe             | DeÄŸer                       |
     | --------------- | --------------------------- |
     | Ä°ÅŸletim Sistemi | Debian (Linux / ARM 64-bit) |
     | Bellek (RAM)    | 4 GB veya Ã¼stÃ¼              |
     | Depolama        | 20 GB VDI                   |
     | AÄŸ AdaptÃ¶rÃ¼     | NATÂ                         |

     > **AÃ§Ä±klama:** Bellek ve depolama, derleme sÃ¼reÃ§leri iÃ§in yeterli olmalÄ±dÄ±r.
   * SSH : `openssh-server` kurup, `ssh kullanÄ±cÄ±@<VM_IP>` ile baÄŸlan
   * link :[https://chatgpt.com/share/685aa279-9c50-800e-aba7-1a678d55750b]
   * link : [https://chatgpt.com/c/686d5030-8cc4-800e-ae51-028f25d8afb6]

2. **Eâ€‘Ticaret Sitesi (Odoo TabanlÄ±)**

   * **Platform:** Odoo Eâ€‘commerce modÃ¼lÃ¼ ile hÄ±zlÄ± ve Ã¶lÃ§eklenebilir geliÅŸtirme
   * **ÃœrÃ¼n KataloÄŸu:** Printed Mug, Printed Tâ€‘Shirt, Printed Bag (kullanÄ±cÄ± tasarÄ±mÄ±yla baskÄ±ya hazÄ±r)
   * **SipariÅŸ AkÄ±ÅŸÄ±:**

     1. KullanÄ±cÄ± tasarÄ±m yÃ¼kler veya seÃ§im yapar
     2. Bizim Ã¶nerilerimizle onaylanan tasarÄ±m baskÄ±ya gÃ¶nderilir
     3. HazÄ±r Ã¼rÃ¼n paketlenip mÃ¼ÅŸteriye kargo ile ulaÅŸtÄ±rÄ±lÄ±r
   * **Logo TasarÄ±mÄ±:**

     * Basit, akÄ±lda kalÄ±cÄ± logo oluÅŸturma
     * SVG ve PNG formatlarÄ±nda optimizasyon
   * **Teknik Entegrasyon:**

     * Odoo modÃ¼lleri: `website_sale`, `website_blog`, `payment_stripe` gibi
     * Git ile sÃ¼rÃ¼m kontrol, CI/CD pipeline (Ã¶r. GitHub Actions)

3. **Odoo Addon GeliÅŸtirme**

   * `odoo_addons/my_module` dizini oluÅŸtur
   * `__manifest__.py`, `models/`, `views/` dosyalarÄ±nÄ± ekle
   * Sanal ortam, `pip install -r requirements.txt`, `./odoo-bin` ile baÅŸlat

4. **Python & TasarÄ±m Desenleri**

   * Kaynak: [Python & Design Patterns Playlist](https://www.youtube.com/playlist?list=PL7yh-TELLS1FuqLSjl5bgiQIEH25VEmIc) ğŸ§
   * AmaÃ§: Magic Methods, Decorator, Generator, OOP, Design Patterns Ã¶ÄŸren

---
## Â 1. Hafta :

* **Understanding Neural Network and Backpropagation Algorithm**

  * *(English)*

    1. [The Perceptron Explained](https://youtu.be/i1G7PXZMnSc?si=45-LrjEc2QzKhU2W)
    2. [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=xkoGPEReRLA_UZ56)
    3. [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=ilfRwZx-0I8Fniu8)
    4. [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=uBw90JvWbqI09VZD)
  * *(Turkish)*

    1. [Neural Network 1 : EÄŸitime ve Kavramlara GiriÅŸ](https://youtu.be/B5MmXmMMuvI?si=JZ4Yfmc_MdsxPyhU)
    2. [Neural Network 2: Perceptron KavramÄ± ve Ã–ÄŸrenme](https://youtu.be/5Lo_HUDtxtw?si=DuZ1y9W11aRfIvrd)
    3. [Neural Network 3: Ã‡ok KatmanlÄ± Yapay Sinir AÄŸlarÄ±](https://youtu.be/qrmaixHBrzU?si=sXHsC0A5XaXsJZId)

* **Download Ollama and choosing model upon the RAM criteria**

  1. [Learn Ollama in 15 Minutes - Run LLM Models Locally for FREE](https://youtu.be/UtSSMs6ObqY?si=xF7yTc1MQEP1tGZh)
  2. [Design Your Own Ollama Model Now!](https://youtu.be/bXf2Cxf3Wk0?si=kwTZ9U8uJjZ1VqZE)

  `ollama run deepseek-r1:8b`

* **Searching about LLM libraries & learning how to customize LLM especially by using Ollama**

  1. [https://github.com/malibayram/llm-from-scratch](https://github.com/malibayram/llm-from-scratch)
  2. [https://www.udemy.com/course/sifirdan-llm-gelistirme-kendi-dil-modelini-olustur/?couponCode=LOCLZDOFFPTRCTRL](https://www.udemy.com/course/sifirdan-llm-gelistirme-kendi-dil-modelini-olustur/?couponCode=LOCLZDOFFPTRCTRL)

* **Downloaded model:**

  `https://ollama.com/library/deepseek-r1 ollama run deepseek-r1:8b`

* **Additional libraries mentioned by the instructor:**

  * [ModelScope](https://github.com/modelscope/modelscope)

  * [Unsloth](https://github.com/unslothai/unsloth)Â (Â JAX tabanlÄ± dataset loader )Â 

  * [Hugging Face](https://huggingface.co/)Â (Transformers)

* Ollama CLI:

  ```bash
  ollama pull deepseek-r1:8b
  ollama run deepseek-r1:8b --context 2048
  ollama fine-tune deepseek-r1:8b --dataset ./data/my_prompts.jsonl --output custom-deepseek
  ```
---

<details> 
  
**<summary> Design Pattern and Python: </summary>**

### 2.0 Magic Methods (Dunder Metotlar)

Python sÄ±nÄ±flarÄ±nÄ±n Ã¶zel davranÄ±ÅŸlarÄ±nÄ± tanÄ±mlayan Ã§ift alt Ã§izgiyle baÅŸlayan metotlardÄ±r.

```python
class Vector:
    def __init__(self, x, y):
        """Nesne oluÅŸturulurken x ve y bileÅŸenlerini atar."""
        self.x, self.y = x, y

    def __add__(self, other):
        """`+` operatÃ¶rÃ¼nÃ¼ Ã¶zelleÅŸtirir: bileÅŸenleri toplayÄ±p yeni Vector dÃ¶ndÃ¼rÃ¼r."""
        return Vector(self.x + other.x, self.y + other.y)

    def __repr__(self):
        """`print(v)` ifadesini `Vector(x, y)` formatÄ±nda gÃ¶sterir."""
        return f"Vector({self.x}, {self.y})"
```

> **AÃ§Ä±klama AÅŸamalarÄ±:**
>
> 1. `__init__` â€“ BaÅŸlatÄ±cÄ± yapÄ±cÄ± metod, x ve y deÄŸerlerini nesneye kaydeder.
> 2. `__add__` â€“ `v1 + v2` iÅŸlemi Ã§aÄŸrÄ±ldÄ±ÄŸÄ±nda devreye girer, yeni `Vector` nesnesi Ã¼retir.
> 3. `__repr__` â€“ Konsolda temiz ve okunabilir Ã§Ä±ktÄ± saÄŸlar.

### 3.0 Decoratorâ€™lar

FonksiyonlarÄ±n giriÅŸine/Ã§Ä±kÄ±ÅŸÄ±na dinamik davranÄ±ÅŸ eklemeye yarar.

#### 3.1 Temel Decorator

```python
def my_decorator(func):
    def wrapper(*args, **kwargs):
        print("ğŸ› ï¸ Ã–ncesi mesaj")
        result = func(*args, **kwargs)
        print("âœ… SonrasÄ± mesaj")
        return result
    return wrapper

@my_decorator
def hello_world():
    print("ğŸ‘‹ Hello World!")

hello_world()
```

> **AÃ§Ä±klama:**
>
> 1. `@my_decorator` ile `hello_world` fonksiyonu `wrapper` ile sarÄ±lÄ±r.
> 2. `wrapper` Ã§alÄ±ÅŸÄ±rken Ã¶nce ek iÅŸlem, sonra orijinal fonksiyon, en son ek iÅŸlem yapÄ±lÄ±r.

#### 3.2 Parametreli Decorator

```python
def repeat(times):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(times):
                print(f"ğŸ”ƒ Ã‡alÄ±ÅŸtÄ±rma {i+1}/{times}")
                func(*args, **kwargs)
        return wrapper
    return decorator

@repeat(3)
def greet(name):
    print(f"ğŸ‰ Merhaba, {name}!")

greet("Besma")
```

> **AÃ§Ä±klama:**
>
> * `repeat(3)` ile decorator fonksiyonuna parametre verilir.
> * `wrapper` iÃ§inde `func` belirtilen kez Ã§aÄŸrÄ±lÄ±r.



#### 4.0 TasarÄ±m Desenleri (Design Patterns)

Kodun modÃ¼lerliÄŸini ve geniÅŸletilebilirliÄŸini artÄ±ran tekrarlanabilir Ã§Ã¶zÃ¼mlerdir.

| Desen AdÄ± | AmaÃ§                                                  |
| --------- | ----------------------------------------------------- |
| Factory   | Nesne oluÅŸturmayÄ± merkezi hale getirir                |
| Proxy     | AsÄ±l nesneye eriÅŸimi kontrol eder                     |
| Singleton | Tekil Ã¶rnek saÄŸlar                                    |
| Composite | AÄŸaÃ§ yapÄ±sÄ±ndaki nesneleri homojen iÅŸlemlerle yÃ¶netir |
| Flyweight | Bellek optimizasyonu yapar                            |

> **Not:** Ã–rnek uygulamalar 9.0 bÃ¶lÃ¼mÃ¼nde detaylÄ± iÅŸlenmiÅŸtir.



#### 5.0 âš™ï¸ Generatorâ€™lar

Bellek kullanÄ±mÄ±nÄ± minimuma indirerek "lazy evaluation" saÄŸlar. Normal `return` yerine `yield` kullanÄ±r.

```python
def cubes(n):
    for i in range(1, n+1):
        yield i**3

for val in cubes(5):
    print(val)  # 1, 8, 27, 64, 125
```

> **AÃ§Ä±klama:**
>
> * `yield` ile deÄŸerler tek tek Ã¼retilir.
> * TÃ¼m liste bellekte tutulmaz.
> * `next()` ile bir sonraki `yield`'e kadar ilerler, tembel deÄŸerlendirme yapÄ±lÄ±r.



#### 6.0 Komut SatÄ±rÄ± ArgÃ¼manlarÄ±

`sys.argv` ve `argparse` ile scriptâ€™lere dÄ±ÅŸarÄ±dan parametre ekler.

```python
import sys, argparse

parser = argparse.ArgumentParser(description="CLI Ã–rneÄŸi")
parser.add_argument("-n", "--name", required=True, help="Ä°sim girin")
args = parser.parse_args()
print(f"Merhaba {args.name}!")
```

> **AÃ§Ä±klama:**
>
> 1. `ArgumentParser` ile parametreler tanÄ±mlanÄ±r.
> 2. `-h` yardÄ±mÄ± otomatik oluÅŸturulur.
> 3. `args.name` ile deÄŸere eriÅŸilir.



#### 7.0 KapsÃ¼lleme & Veri Gizleme

SÄ±nÄ±f iÃ§i verileri korur ve dÄ±ÅŸ eriÅŸimi kontrol eder.

```python
class Person:
    def __init__(self, name):
        self.__name = name  # private attribute

    @property
    def name(self):
        return self.__name

    @name.setter
    def name(self, value):
        if len(value) < 2:
            raise ValueError("Ä°sim en az 2 karakter olmalÄ±.")
        self.__name = value
```

> **AÃ§Ä±klama:**
>
> * `__name` ile doÄŸrudan eriÅŸim engellenir.
> * `@property` ve `@setter` ile kontrollÃ¼ eriÅŸim saÄŸlanÄ±r.



#### 8.0 Type Hinting

Kod okunabilirliÄŸini ve statik analiz desteÄŸini artÄ±rÄ±r.

```python
from typing import List

def toplam(liste: List[int]) -> int:
    return sum(liste)

print(toplam([1, 2, 3]))
```

> **AÃ§Ä±klama:**
>
> * `List[int]` ve `-> int` bildirimi not niteliÄŸindedir.
> * `mypy` gibi araÃ§larla hatalar Ã¶nceden yakalanabilir.


### 9.0 DetaylÄ± TasarÄ±m Desenleri

#### 9.1 Factory Pattern

```python
class IPerson:
    def get_role(self): pass
class Student(IPerson):
    def get_role(self): return "Student"
class Teacher(IPerson):
    def get_role(self): return "Teacher"
class PersonFactory:
    @staticmethod
    def build(person_type):
        if person_type == "Student": return Student()
        if person_type == "Teacher": return Teacher()
        raise ValueError("GeÃ§ersiz tip")
```

> **AÃ§Ä±klama:**
>
> * `PersonFactory.build()` ile nesne tÃ¼rÃ¼ runtime'da belirlenir.

</details>

<details> 
  **<summary> ğŸ› ï¸ Odoo on Debian (Manual Kurulum)</summary>**
  
**DetaylÄ± AdÄ±mlar:**

1. Sistem GÃ¼ncelleme ve BaÄŸÄ±mlÄ±lÄ±klar

```bash
sudo apt update && sudo apt upgrade -y
sudo timedatectl set-timezone Europe/Istanbul
sudo apt install git python3.10 python3.10-venv python3-pip build-essential wget libpq-dev libxml2-dev libxslt1-dev libjpeg-dev libblas-dev libatlas-base-dev -y
```

2. Odoo KullanÄ±cÄ±sÄ± & PostgreSQL

```bash
sudo useradd -r -s /bin/bash -m -d /opt/odoo odoo
sudo -u postgres createuser --createdb --username postgres odoo
```

3. Kaynak Kodunun Ä°ndirilmesi

```bash
sudo -u odoo git clone --depth 1 --branch 16.0 https://github.com/odoo/odoo.git /opt/odoo/odoo
```

4. Python Sanal OrtamÄ± & Gereksinimler

```bash
sudo -u odoo python3.10 -m venv /opt/odoo/venv
source /opt/odoo/venv/bin/activate
pip install --upgrade pip setuptools wheel
pip install -r /opt/odoo/odoo/requirements.txt
deactivate
```

5. KonfigÃ¼rasyon DosyasÄ± (/etc/odoo.conf)

```ini
[options]
db_user = odoo
db_password = odoo
addons_path = /opt/odoo/odoo/addons
admin_passwd = YourStrongAdminPassword
xmlrpc_port = 8069
logfile = /var/log/odoo/odoo.log
```

```bash
sudo chown odoo: /etc/odoo.conf
sudo chmod 640 /etc/odoo.conf
```

6. Log & Veri KlasÃ¶rleri

```bash
sudo mkdir -p /var/log/odoo /var/lib/odoo
sudo chown -R odoo: /var/log/odoo /var/lib/odoo
```

7. Systemd Servis

```ini
[Unit]
Description=Odoo16 Service
After=network.target postgresql.service

[Service]
Type=simple
User=odoo
Group=odoo
ExecStart=/opt/odoo/venv/bin/python3 /opt/odoo/odoo/odoo-bin -c /etc/odoo.conf
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
```

```bash
sudo tee /etc/systemd/system/odoo.service > /dev/null <<EOF
[Unit]
Description=Odoo16 Service
After=network.target postgresql.service

[Service]
Type=simple
User=odoo
Group=odoo
ExecStart=/opt/odoo/venv/bin/python3 /opt/odoo/odoo/odoo-bin -c /etc/odoo.conf
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable odoo.service
sudo systemctl start odoo.service
```

8. GÃ¼venlik DuvarÄ±

```bash
sudo ufw allow 8069/tcp
```
</details>
<details> **<summary> ğŸ”‘ SSH & VS Code Remote </summary>**

1. Debian VM Ãœzerinde SSH Sunucusu

```bash
sudo apt update
sudo apt install openssh-server -y
sudo systemctl enable --now ssh
```

2. SSH Anahtar Ã‡ifti OluÅŸturma (macOS)

```bash
ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519 -N ''
```

3. AnahtarÄ±n VMâ€™e KopyalanmasÄ±

```bash
ssh-copy-id -i ~/.ssh/id_ed25519.pub odoo@<VM_IP>
```

4. SSH KonfigÃ¼rasyonu (macOS \~/.ssh/config)

```ini
Host debian-odoo
  HostName <VM_IP>
  User odoo
  IdentityFile ~/.ssh/id_ed25519
  ServerAliveInterval 60
  ControlMaster auto
  ControlPath ~/.ssh/cm-%r@%h:%p
```

5. BaÄŸlantÄ±yÄ± Test Etme

```bash
ssh debian-odoo -v
```

6. Port YÃ¶nlendirme (Tunnel)

```bash
ssh -L 8069:localhost:8069 debian-odoo
```

7. VS Code Remote - SSH

8. VS Code eklentisini yÃ¼kle: Remote - SSH

9. F1 > Remote-SSH: Add New SSH Host > debian-odoo

10. Remote Explorerâ€™dan debian-odooâ€™ya baÄŸlan

11. /opt/odoo dizinini aÃ§

12. Ek Ä°puÃ§larÄ±

* Agent Forwarding: ForwardAgent yes
* Multiplexing: ControlPersist 600

</details>


# ğŸŒ Neural Network and Backpropagation Algorithm ğŸ¤–ğŸ§ 
![Biyolojik ve Yapay NÃ¶ron](https://github.com/user-attachments/assets/17b7f50a-632a-480e-9968-2de823137be0)
- <caption><b>Åekil 1.</b> Sol: Ä°nsan beynindeki biyolojik nÃ¶ron yapÄ±sÄ±. SaÄŸ: Benzer iÅŸlevleri taklit eden yapay sinir aÄŸÄ± mimarisi.</caption>

![Biyolojik ve Yapay NÃ¶ron](https://github.com/user-attachments/assets/8bf57052-03b2-46e5-9918-24f403e1ac0b)
- <caption><b>Åekil 2.</b> (a) Biyolojik nÃ¶ronun yapÄ±sÄ±, (b) Yapay nÃ¶ronun matematiksel modeli, (c) Ã‡ok katmanlÄ± yapay sinir aÄŸÄ±.</caption>

### 1. NÃ¶ron Nedir? âš™ï¸
- **TanÄ±m:** SayÄ± tutan birimdir ve her nÃ¶ron 0â€“1 arasÄ± bir aktivasyon deÄŸeri taÅŸÄ±r.  
- **GiriÅŸ katmanÄ±:** 784 nÃ¶ron (28Ã—28 piksellik her piksel iÃ§in bir nÃ¶ron)

### 2. Aktivasyon Nedir? ğŸŒŸ
- **TanÄ±m:** Pikselin gri ton deÄŸeridir (0 = siyah, 1 = beyaz).  
- **AnlamÄ±:** YÃ¼ksek aktivasyon = o nÃ¶ron â€œparlakâ€ (aktiftir)

### 3. Problemin TanÄ±mÄ± ğŸ§
28Ã—28 piksellik dÃ¼ÅŸÃ¼k Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ el yazÄ±sÄ± rakam gÃ¶rÃ¼ntÃ¼lerini (Ã¶rneÄŸin â€œ3â€) bilgisayarla otomatik tanÄ±manÄ±n ne kadar zor olduÄŸunu vurguluyor.  
> **Ä°nsan Beyniâ€“Bilgisayar KarÅŸÄ±laÅŸtÄ±rmasÄ±:** Ä°nsan gÃ¶rsel korteksi bu gÃ¶revi zorlanmadan Ã§Ã¶zer; bilgisayarda ise â€œkomik derecedeâ€ karmaÅŸÄ±k hale gelir.

### 4. Ã–ÄŸrenme (Learning) KavramÄ± ğŸš€
Soyut bir ÅŸeyi somutlaÅŸtÄ±rmak sonucu gerÃ§ekleÅŸir.  
**AmaÃ§:** On binlerce parametrenin â€œdoÄŸruâ€ deÄŸerlerini otomatik ve hÄ±zlÄ± bir ÅŸekilde bulmak.


### 5. Soyutlama DÃ¼zeyleri ğŸ—ï¸
- **GiriÅŸ KatmanÄ± (Input Layer):** Ham pikseller. 784 nÃ¶ron.  
- **Gizli Katmanlar (Hidden Layers):** Kenar, kÃ¶ÅŸe, dÃ¶ngÃ¼ gibi alt-bileÅŸenler.  
  - Ã–rnekte iki gizli katman, her biri 16 nÃ¶ron.  
- **Ã‡Ä±kÄ±ÅŸ KatmanÄ± (Output Layer):** BileÅŸen kombinasyonlarÄ±ndan rakam tanÄ±masÄ±. 10 nÃ¶ron (0â€“9).  

> **Genel AmaÃ§:** AynÄ± yapÄ±, farklÄ± gÃ¶rÃ¼ntÃ¼ ve ses tanÄ±ma gÃ¶revlerine de uyarlanabilir.

### 6. Ä°leri Besleme MekanizmasÄ± ğŸ”„
- Her gizli katmandaki nÃ¶ron, Ã¶nceki katmandaki tÃ¼m nÃ¶ronlarÄ±n aktivasyonlarÄ±yla â€œbaÄŸlantÄ±lÄ±dÄ±râ€.  
- **AÄŸÄ±rlÄ±k (weight):** Sinyallerin gÃ¼cÃ¼nÃ¼ belirler.  
- **Bias:** NÃ¶ronun â€œne zamanâ€ aktif olacaÄŸÄ±nÄ± kontrol eden eÅŸik ayarÄ±.  

> EÄŸitim aÅŸamasÄ±nda bias ve weight parametreleri gradient descent ile otomatik olarak ayarlanÄ±r.

### 7. Parametre HesabÄ± ğŸ“Š
- Her katman atlamasÄ± iÃ§in *Ã¶nceki katmandaki nÃ¶ron sayÄ±sÄ±* Ã— *sonraki katmandaki nÃ¶ron sayÄ±sÄ±* kadar baÄŸlantÄ± (weight).  
- **Aktivasyon fonksiyonu:** Toplam sonucu 0â€“1 aralÄ±ÄŸÄ±na sÄ±kÄ±ÅŸtÄ±rmak (sigmoid veya ReLU).  
- **Toplam parametre sayÄ±sÄ±:** weight sayÄ±sÄ± + bias sayÄ±sÄ±.  

| GeÃ§iÅŸ                          | Ã–nceki katman | Sonraki katman | AÄŸÄ±rlÄ±k Matrisi Boyutu | AÄŸÄ±rlÄ±k sayÄ±sÄ±   | Bias sayÄ±sÄ± | Parametre | Bias vektÃ¶rÃ¼ boyutu | Ara Toplam |
| ------------------------------ | ------------- | -------------- | ---------------------- | ---------------- | ----------- | --------- | ------------------- | ---------- |
| GiriÅŸ â†’ 1. Gizli katman        | 784 nÃ¶ron     | 16 nÃ¶ron       | (16, 784)              | 784 Ã— 16 = 12 544 | 16          | 12 560    | (16, 1)             | 12 560     |
| 1. Gizli katman â†’ 2. Gizli katman | 16 nÃ¶ron  | 16 nÃ¶ron       | (16, 16)               | 16 Ã— 16 = 256     | 16          | 272       | (16, 1)             | 272        |
| 2. Gizli katman â†’ Ã‡Ä±kÄ±ÅŸ        | 16 nÃ¶ron      | 10 nÃ¶ron       | (10, 16)               | 16 Ã— 10 = 160     | 10          | 170       | (10, 1)             | 170        |
| **Genel Toplam**               |               |                |                        | **12 960**        | **42**      | **13 002**|                     | **13 002** |


### 8. Backpropagationâ€™Ä±n AmacÄ± ğŸ¯
- **AmaÃ§:** Hangi aÄŸÄ±rlÄ±k, hatayÄ± ne kadar etkiliyor?  
- **Hedef:** Modelin tahmin hatasÄ±nÄ± (maliyeti) en aza indirmek.  
> **Ã–zet:** Tahmin â†’ Hata â†’ Gradyan â†’ GÃ¼ncelleme

![Gradient Descent](https://github.com/user-attachments/assets/fc6ecc38-c3a6-4a49-ae62-0e28e6e3a4bf)
<caption><b>Åekil 1.</b> Gradyan yÃ¶nÃ¼nde (tÃ¼rev iÅŸareti boyunca) kÃ¼Ã§Ã¼k adÄ±mlarla aÄŸÄ±rlÄ±klarÄ±n gÃ¼ncellenmesi ve maliyetin (cost) en aza indirilmesi.</caption>

### 9. AdÄ±m AdÄ±m Ä°ÅŸleyiÅŸ ğŸš¶â€â™€ï¸
| AdÄ±m | Ne YapÄ±yoruz?                                                                                 |
| ---- | --------------------------------------------------------------------------------------------- |
| 1    | **Tahmin (Forward Pass):** Girdiyi aÄŸdan geÃ§irip Ã§Ä±ktÄ± deÄŸerini hesaplÄ±yoruz.                  |
| 2    | **Hata Hesaplama (Loss):** Tahmin ile gerÃ§ek etiket arasÄ±ndaki farkÄ± Ã¶lÃ§Ã¼yoruz (Ã¶rn. kare fark). |
| 3    | **Geri YayÄ±lÄ±m (Backprop):** HatanÄ±n her aÄŸÄ±rlÄ±ÄŸa ne kadar etki ettiÄŸini belirliyoruz.         |
| 4    | **AÄŸÄ±rlÄ±k GÃ¼ncelleme:** AÄŸÄ±rlÄ±klarÄ±, hatayÄ± azaltacak yÃ¶nde kÃ¼Ã§Ã¼k adÄ±mlarla gÃ¼ncelliyoruz.      |

### 10. Basit Ã–rnek GÃ¶rseliyle ğŸ–¼ï¸
- **Girdi:** `[xâ‚, xâ‚‚]`  
- **AÄŸÄ±rlÄ±klar:** `[wâ‚, wâ‚‚]`, **Bias:** `b`  
- **Hesap:** `z = wâ‚Â·xâ‚ + wâ‚‚Â·xâ‚‚ + b` â†’ `a = sigmoid(z)`  
- **Hata:** `(a âˆ’ y)Â²` (`y` = gerÃ§ek etiket)  
- **Geri YayÄ±lÄ±m:** Hangi `wâ‚` veya `wâ‚‚` deÄŸiÅŸirse hata ne kadar deÄŸiÅŸir?  
- **GÃ¼ncelleme:** `w â† w âˆ’ Î·Â·(etki)` (`Î·` = Ã¶ÄŸrenme hÄ±zÄ±)

### 11. DÃ¶ngÃ¼yÃ¼ Tekrarlama ğŸ”
Her veri noktasÄ± veya mini-batch iÃ§in:  
1. Tahmin  
2. Hata Hesaplama  
3. Geri YayÄ±lÄ±m  
4. AÄŸÄ±rlÄ±k GÃ¼ncelleme  

> Bu dÃ¶rt adÄ±m tekrarlanarak aÄŸ â€œÃ¶ÄŸrenirâ€ ve tahmin doÄŸruluÄŸu artar.


![Loss Landscape](https://github.com/user-attachments/assets/444665a0-637e-4487-b34c-ff7d31048697)
<caption><b>Åekil 2.</b> Ã‡ok boyutlu kayÄ±p yÃ¼zeyinde (loss landscape) optimizasyonun izlediÄŸi yol; yerel minimumlar, eyer noktalarÄ± ve zor bÃ¶lgeler.</caption>



#### ğŸ“š Ek Kaynaklar
- **Ian Goodfellow, Yoshua Bengio & Aaron Courville** â€“ *Deep Learning* (MIT Press, 2016)  
  ResmÃ® web sitesi ve PDF: https://www.deeplearningbook.org/  
- **Michael Nielsen** â€“ *Neural Networks and Deep Learning* (online kitap, 2015)  
  EtkileÅŸimli alÄ±ÅŸtÄ±rmalar: https://neuralnetworksanddeeplearning.com/  
- **Stanford CS231n** â€“ *Convolutional Neural Networks for Visual Recognition*  
  Ders notlarÄ±: http://cs231n.stanford.edu/  
  Video dersleri: https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv  


## ğŸŒ Fundamental of Network:

### [The Book That We Used ğŸ“˜](https://drive.google.com/file/d/1RzrRfISQd7i8XpL0zkMRHvKRFqIOs7df/view?usp=drive_link)  & My Notes :

### ğŸ§± **OSI MODEL**
**P**LEASE **D**O **N**OT **T**HROW THE **S**OUSAGE **P**Ä°ZZA **A**WAY  

<details>
**<summary> as Table</summary> **

| Katman No | Katman AdÄ±      | AÃ§Ä±klama                                                                 | Ã–rnek DonanÄ±m / Protokoller         |
|-----------|------------------|--------------------------------------------------------------------------|-------------------------------------|
| 1ï¸âƒ£        | **Physical**     | Bit seviyesinde iletim; elektriksel ve fiziksel Ã¶zellikler               | ğŸ§± Hub, Kablo, Voltaj, Bit           |
| 2ï¸âƒ£        | **Data Link**    | Frame oluÅŸturma, MAC adresleme ve hata denetimi                         | ğŸ§  Switch, MAC, Ethernet             |
| 3ï¸âƒ£        | **Network**      | IP adresleme, yÃ¶nlendirme ve paketleme                                  | ğŸ§­ Router, IP, ICMP                  |
| 4ï¸âƒ£        | **Transport**    | UÃ§tan uca iletiÅŸim, veri aktarÄ±m kontrolÃ¼                               | ğŸ“¡ TCP, UDP, Port NumaralarÄ±         |
| 5ï¸âƒ£        | **Session**      | Oturum aÃ§ma ve senkronizasyon                                           | ğŸ—‚ï¸ Oturum YÃ¶netimi, RPC, NetBIOS     |
| 6ï¸âƒ£        | **Presentation** | Verinin sunumu, ÅŸifreleme, biÃ§im dÃ¶nÃ¼ÅŸÃ¼mleri                            | ğŸ” JPEG, MP3, SSL/TLS, ASCII         |
| 7ï¸âƒ£        | **Application**  | KullanÄ±cÄ±nÄ±n doÄŸrudan etkileÅŸtiÄŸi katman                                | ğŸŒ HTTP, FTP, SMTP, GitHub API       |

</details>

ğŸ“¦ 1->3 : Media Layerâ€ƒâ€ƒğŸ’» 4->7 : Host Layer  
1. **Physical** âš¡ï¸: bits, hub  
2. **Data Link** ğŸ”—: switcher, frames, MAC address  
3. **Network** ğŸŒ: router, packages, IP address  
4. **Transport** ğŸšš:
   - **TCP** ğŸ”µ:
     - âœ… %100 all bits delivered  
     - ğŸ“¥ make sure if it's received  
     - ğŸ¤ 3 way handshake : SYN â SYN-ACK â ACK
     
   - **UDP** ğŸ”´:
     - ğŸ“ phone call  
     - â±ï¸ real time  
     - â“ not sure if all bits received  
5. **Session** ğŸ—‚ï¸:
   - ğŸ§  Logical Parts  
   - ğŸ” Synchronization and send to ports  
6. **Presentation** ğŸ­:
   - ğŸ” connect the data to decrypt or encrypt (if it's needed)  
   - ğŸ§¬ syntax layer  
7. **Application** ğŸ§‘â€ğŸ’»:
   - ğŸ§¾ End the user layer  
   - ğŸ”š Deal with last protocol  
---

### Ä°nternet GerÃ§ekte NasÄ±l Ã‡alÄ±ÅŸÄ±r?

[Video](https://www.youtube.com/watch?v=GIvU5pDrT1o) 

Ä°nternete baÄŸlanmak, istemci (client) cihazdan baÅŸlayan ve uÃ§tan uca bir aÄŸ altyapÄ±sÄ±nÄ± tetikleyen, Ã§ok katmanlÄ± bir iletiÅŸim sÃ¼recidir. KullanÄ±cÄ± bir istekte bulunduÄŸunda (Ã¶rneÄŸin YouTube'da bir videoya tÄ±klamak), bu istek IP paketlerine bÃ¶lÃ¼nerek DNS sunucusu Ã¼zerinden hedef sunucunun IP adresi Ã§Ã¶zÃ¼lÃ¼r ve rotalanÄ±r. Veriler, genellikle yÃ¼ksek performanslÄ± SSDâ€™lerle donatÄ±lmÄ±ÅŸ, redundant sistemlere sahip veri merkezlerinde saklanÄ±r. Bu veriler, dÃ¼nya genelini saran denizaltÄ± fiber optik kablolar Ã¼zerinden, Ä±ÅŸÄ±k sinyalleri ÅŸeklinde minimum gecikmeyle iletilir. Fiber hatlardan Ã§Ä±kan optik sinyaller, modemlerde elektriksel sinyale dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lerek WiFi veya baz istasyonlarÄ± Ã¼zerinden son kullanÄ±cÄ±ya ulaÅŸÄ±r. IP yÃ¶nlendirme, paket sÄ±ralamasÄ± ve hata kontrolÃ¼ ise TCP/IP baÅŸta olmak Ã¼zere Ã§eÅŸitli aÄŸ protokolleri tarafÄ±ndan saÄŸlanÄ±r. Her veri paketi, hedefe farklÄ± yollardan ulaÅŸabilir ve istemci tarafÄ±nda yeniden birleÅŸtirilerek iÃ§erik oluÅŸturulur. Bu sistem, ICANN gibi kÃ¼resel otoriteler tarafÄ±ndan yÃ¶netilen bir adresleme ve protokol standardizasyonu ile iÅŸler; yani bir video akÄ±ÅŸÄ±, aslÄ±nda Ä±ÅŸÄ±k hÄ±zÄ±nda hareket eden binlerce paketin, onlarca protokol ve altyapÄ± katmanÄ±yla senkronize ÅŸekilde taÅŸÄ±nmasÄ±dÄ±r.


---

### ğŸš€ Customize LLM 

Curriculum :
- Module 0: Introduction & Environment Setup
  * Lesson 0.1 â€” Welcome to the LLM Revolution
  *  Course goals, what we will build
  * Why learn LLMs from scratch?
  * Open-source vs closed-source models (GPT-4 vs LLaMA 3)
  * Lesson 0.2 - Core Concepts: Autoregression, Transformers, Pretraining vs Fine-tuning
  * Lesson 0.3 - Setting Up Your Deep Learning Environment
  * Python, PyTorch, datasets, tiktoken, transformers
  * GPU on Colab / Kaggle
- Module 1: Data - The Fuel for LLMs
  *  Lesson 1.1 - Understanding Text & The Role of Tokenization
  *  Words, subwords, characters
  *  BPE explained
  *  Lesson 1.2 â€” Practical Tokenization with tiktoken
  *  Encoding/decoding tokens
  *  Vocabulary size, special tokens

````markdown

# ğŸš€ macOS Ä°Ã§in Pyenv Kurulumu

> **Ã–neri:** Homebrew, macOSâ€™ta pyenvâ€™in temel kullanÄ±mÄ±nÄ± kolaylaÅŸtÄ±rÄ±r.

## ğŸ› ï¸ 1. Homebrew GÃ¼ncelleme ve Pyenv Kurulumu

```bash
# Homebrewâ€™Ã¼ gÃ¼nceller
brew update

# Pyenvâ€™i yÃ¼kler
brew install pyenv
````

##### ğŸ”­ 2. GeliÅŸtirme BaÅŸÄ±na (Development Head) Pyenv Kurulumu *(Opsiyonel)*

```bash
# En son geliÅŸtirme sÃ¼rÃ¼mÃ¼nÃ¼ kurar
brew install pyenv --head
```

##### ğŸ”§ 3. Kabuk OrtamÄ±nÄ± Ayarlama

```bash
# AÅŸaÄŸÄ±daki satÄ±rlarÄ± ~/.bashrc veya ~/.zshrc dosyanÄ±za ekleyin
if command -v pyenv 1>/dev/null 2>&1; then
  # login shell iÃ§in
  eval "$(pyenv init --path)"
  # interactive shell iÃ§in
  eval "$(pyenv init -)"
fi
```


##### ğŸ›¡ï¸ 4. `brew doctor` UyarÄ±sÄ±nÄ± Giderme *(Opsiyonel)*

> BazÄ± formÃ¼ller Pythonâ€™a link ederken yanlÄ±ÅŸlÄ±kla pyenv tarafÄ±ndan saÄŸlanan sÃ¼rÃ¼mÃ¼ kullanÄ±rsa uyarÄ± alÄ±rsÄ±nÄ±z.

* **Bash/Zsh iÃ§in:**

```
      alias brew='env PATH="${PATH//$(pyenv root)\/shims:/}" brew'
```

* **Fish iÃ§in:**

  ```
      alias brew="env PATH=(string replace (pyenv root)/shims '' \"$PATH\") brew"
  ```

> **Kaynak:**
> [pyenv Â· GitHub â€“ Homebrew in macOS](https://github.com/pyenv/pyenv?tab=readme-ov-file#homebrew-in-macos)

---

## Macte Ollama, Cpu ve Gpu arasÄ±ndaki baÄŸlantÄ±yÄ± anlamak : 
> Apple Silicon (M1/M2/M3) Ã¼zerinde NVIDIAâ€™ya Ã¶zgÃ¼ nvtop veya top -o gpu gibi araÃ§lar Ã§alÄ±ÅŸmadÄ±ÄŸÄ± iÃ§in GPU kullanÄ±mÄ±nÄ± CLIâ€™dan izlemek iÃ§in :
#### 1. GUI: Activity Monitor â€“ GPU History

1. **Activity Monitor**â€™Ã¼ aÃ§Ä±n (`Finder` â†’ `Applications` â†’ `Utilities` â†’ **Activity Monitor**).
2. MenÃ¼den **Window** â†’ **GPU History** seÃ§eneÄŸini seÃ§in.
3. Burada entegre GPUâ€™nuzun anlÄ±k yÃ¼kÃ¼nÃ¼ grafiksel olarak gÃ¶rebilirsiniz.

> AÃ§Ä±klama: En hÄ±zlÄ± ve en dolaysÄ±z yÃ¶ntemdir; root izni veya ekstra kurulum gerekmez.

#### 2. CLI: `powermetrics` ile Tek Seferlik AnlÄ±k Ã–lÃ§Ã¼m

Terminalâ€™den aÅŸaÄŸÄ±daki komut, 0.5 saniyelik bir Ã¶rnekleme ile GPU â€œactive residencyâ€ (yani GPUâ€™nun ne oranda meÅŸgul olduÄŸuna) dair tek seferlik bir anlÄ±k gÃ¶rÃ¼ntÃ¼ Ã§Ä±karÄ±r:

```sudo powermetrics --samplers gpu_power -i500 -n1 \ | grep "GPU active residency"```

> AÃ§Ä±klama adÄ±m adÄ±m:
> 
> - `sudo`: root izni, Ã§Ã¼nkÃ¼ `powermetrics` sistem seviyesinde Ã§alÄ±ÅŸÄ±r.
> - `-samplers gpu_power`: yalnÄ±zca GPU gÃ¼Ã§ kullanÄ±m istatistiklerini toplar.
> - `i500`: 500 ms (0.5 s) Ã¶rnekleme aralÄ±ÄŸÄ±.
> - `n1`: tek Ã¶rnek alÄ±p komuttan Ã§Ä±kar.
> - `grep "GPU active residency"`: yalnÄ±zca yÃ¼zde deÄŸeri iÃ§eren satÄ±rÄ± filtreler. ([Stack Overflow](https://stackoverflow.com/questions/63881791/macos-get-gpu-history-usage-from-terminal?utm_source=chatgpt.com), [OS X Daily](https://osxdaily.com/2024/07/05/how-to-see-individual-core-cpu-usage-on-mac-with-powermetrics/?utm_source=chatgpt.com))


#### 3. CLI: `asitop` ile CanlÄ± Terminal ArayÃ¼zÃ¼

`asitop`, `powermetrics`â€™i arka planda kullanarak renkli, sÃ¼rekli gÃ¼ncellenen bir terminal arayÃ¼zÃ¼ sunar.

##### 1. **PATH** sorununuzu Ã§Ã¶zmek iÃ§in (eÄŸer hÃ¢lÃ¢ `command not found` alÄ±yorsanÄ±z):
    
 ```
    echo 'export PATH="$HOME/Library/Python/3.9/bin:$PATH"' >> ~/.zshrc
    source ~/.zshrc
 ```
    
##### 2. ArdÄ±ndan Ã§alÄ±ÅŸtÄ±rÄ±n:
    
  ```
    sudo asitop
   ```
    

> AÃ§Ä±klama:
> - `asitop`, CPU (â€œE-clusterâ€/â€œP-clusterâ€), GPU (entegre), ANE (Apple Neural Engine) ve bellek kullanÄ±mÄ±nÄ± ayrÄ± sÃ¼tunlarda gÃ¶sterir.
> - SÃ¼rekli gÃ¼ncelleme ve grafiksel Ã§ubuklar sayesinde takip etmesi kolaydÄ±r. ([GitHub](https://github.com/tlkh/asitop?utm_source=chatgpt.com))


### Terminale **top -o cpu** yazarak CPU hareketlerini gÃ¶rebilirsiniz.



NOT : YUKARIYA nÃ¶roloji sinaps fln fotoÄŸrafÄ± eklenmeli .. murat hocanÄ±n dediÄŸini hatÄ±rla ..

---





## ğŸ“š LLMâ€™in Temel UnsurlarÄ±

1. **Veri KÃ¼mesinin Hacmi**  
   - Modelin â€œÃ¶ÄŸreneceÄŸiâ€ Ã¶rnek sayÄ±sÄ±; genellikle milyarlarca token.  
2. **Transformer Mimarisi**  
   - Modelin katman yapÄ±sÄ±, dikkat (attention) mekanizmalarÄ±, besleme (feedâ€‘forward) bloklarÄ±.

---

## ğŸ“š Transformer Mimarisi

Transformer, tÃ¼m cÃ¼mleye aynÄ± anda bakar ve kelimeler arasÄ±ndaki baÄŸlarÄ± â€œattentionâ€ ile Ã¶ÄŸrenir.  
Daha detaylÄ± bilgi biÃ§in lÃ¼tfen makaleyi inceleyin:  
[Vaswani et al., â€œAttention is All You Needâ€](https://github.com/user-attachments/files/21395756/NIPS-2017-attention-is-all-you-need-Paper.pdf)

![Transformer ÅemasÄ±](https://github.com/user-attachments/assets/2df429b7-930b-407b-942e-33574005f8db)

---

## Temel BileÅŸenler

- **Tokenizasyon**  
  Ham metni, anlamlÄ± parÃ§alara ayÄ±rma iÅŸlemi.  
- **Embedding**  
  Tokenâ€™larÄ± vektÃ¶rlere (sayÄ± dizilerine) dÃ¶nÃ¼ÅŸtÃ¼rme.  
- **Positional Encoding**  
  Tokenâ€™larÄ±n pozisyon bilgilerini modele ekleme.  
- **Attention (Dikkat)**  
  Hangi tokenâ€™Ä±n hangi tokenâ€™la ne kadar iliÅŸkili olduÄŸunu hesaplama.  
- **Katman (Layer)**  
  Bilgiyi derinleÅŸtiren, sÄ±rayla uygulanan bloklar.  
- **Feedâ€‘Forward**  
  Her katmanda bilinÃ§li Ã¶ÄŸrenmeyi pekiÅŸtiren tam baÄŸlÄ± (dense) aÄŸ bloÄŸu.  

â€”
<img width="2385" height="1179" alt="image" src="https://github.com/user-attachments/assets/4b88b1fc-542d-4769-9bb0-f5ac0b810163" />

## ğŸ’¡ Tokenizasyon Nedir?
â€œToken is an integer that represents a character, or a short segment of charactersâ€¦â€
Ham metni, modelin â€œanlayabileceÄŸiâ€ kÃ¼Ã§Ã¼k parÃ§alara (token) bÃ¶lme iÅŸlemidir.  
Ã–rnek:  
> â€œMerhaba dÃ¼nyaâ€ â†’ `[â€œMerhabaâ€, â€œdÃ¼nyaâ€]` veya `[â€œMerâ€, â€œhabaâ€, â€œdÃ¼nâ€, â€œyaâ€]`

### Tipleri

#### 1. Word-Level Tokenization (Kelime DÃ¼zeyi)

```python
def word_level_tokenize(text):
    # Metni boÅŸluklardan bÃ¶lerek kelime dizisi dÃ¶ner
    return text.split()

print(word_level_tokenize("Merhaba dÃ¼nya, bugÃ¼n nasÄ±lsÄ±n?"))
# â†’ ['Merhaba', 'dÃ¼nya,', 'bugÃ¼n', 'nasÄ±lsÄ±n?']
```

AÃ§Ä±klama:
BoÅŸluklarÄ± ayÄ±rÄ±r.
Noktalama iÅŸaretleri ayrÄ± token olarak kalabilir.
Basit ama nadir kelimelerde sÃ¶zlÃ¼k dÄ±ÅŸÄ± (OOV) sorunu yaÅŸanÄ±r.
2. Character-Level Tokenization (Karakter DÃ¼zeyi)
def character_level_tokenize(text):
    # Metni karakter bazlÄ± listeye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
    return list(text)

print(character_level_tokenize("Merhaba"))
# â†’ ['M', 'e', 'r', 'h', 'a', 'b', 'a']

AÃ§Ä±klama:
Her harfi ayrÄ± token olarak iÅŸler.
OOV sorunu yoktur ama sÄ±ralÄ± anlam Ã§Ä±karÄ±mÄ± zorlaÅŸÄ±r.
3. Subword Tokenization (Alt-ParÃ§a DÃ¼zeyi)
Kelimeyi, hem anlamlÄ± hem de sÄ±k gÃ¶rÃ¼len parÃ§alara bÃ¶ler.
Ã–rnek: â€œbilinmeyenâ€ â†’ [â€œâ–bilâ€, â€œinâ€, â€œmeâ€, â€œyenâ€]
Avantaj:
Daha Ã¶nce gÃ¶rÃ¼lmemiÅŸ kelimelerle karÅŸÄ±laÅŸÄ±ldÄ±ÄŸÄ±nda parÃ§alar Ã¼zerinden genel anlam tahmini yapÄ±labilir.

Subword Tokenizasyon AlgoritmalarÄ±
Byteâ€‘Pair Encoding (BPE) : Dil bilgisini gÃ¶z ardÄ± eder, sadece istatistiksel tekrar sÄ±klÄ±ÄŸÄ±na dayanÄ±r.  
    KullanÄ±ldÄ±ÄŸÄ± Modeller: GPTâ€‘2, GPTâ€‘3 vb.
    AdÄ±mlar:
-      TÃ¼m kelimeleri harf harf ayÄ±rÄ±n.
-      En sÄ±k tekrar eden harf Ã§iftini bulun.
-      Bu Ã§ifti birleÅŸtirin, yeni token ekleyin.
Tekrar ederek istenen sÃ¶zlÃ¼k boyutuna ulaÅŸÄ±n.

Ã–rnek: Hugging Face GPTâ€‘2 Tokenizer
from transformers import GPT2Tokenizer

# GPT-2 Ã¶nceden eÄŸitilmiÅŸ tokenizasyon kÃ¼tÃ¼phanesini yÃ¼kler
```
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

text = "bilinmeyen kelime"
tokens = tokenizer.tokenize(text)
ids = tokenizer.convert_tokens_to_ids(tokens)

print("Tokens:", tokens)
print("IDs:", ids)
```
AÃ§Ä±klama:
tokenize() metni altâ€‘parÃ§alara bÃ¶ler.
convert_tokens_to_ids() her bir tokenâ€™a sayÄ±sal ID atar.
Bu sayede model, nÃ¼merik verilerle iÅŸlem yapabilir.

---

## Context Length AltÄ±nda Padding

Bir dil modeli eÄŸitimi sÄ±rasÄ±nda girilen dizi, modelin `context_length` parametresinden (Ã¶rn. 512 token) kÄ±sa ise:

- **Ã‡Ã¶zÃ¼m**: Dizinin sonuna Ã¶zel `<PAD>` tokenâ€™larÄ± eklenir.  
- **Neden?**: Modelin sabit uzunlukta giriÅŸ beklemesi nedeniyle; aynÄ± boyda tensÃ¶rler oluÅŸturulur.  

*Bir dil modeli eÄŸitimi sÄ±rasÄ±nda girilen dizinin context lengthâ€™ten kÄ±sa olduÄŸu durumlarda ne yapÄ±lÄ±r?*

      - Dizinin sonuna padding tokenlari eklenir.


## MNIST Data Set

- **Kaynak**: [Kaggle â€“ MNIST Handwritten Digits Classification][1]  
- **Ã–zellik**: 60.000 eÄŸitim, 10.000 test Ã¶rneÄŸi; 28Ã—28 gri tonlamalÄ± el yazÄ±sÄ± rakam gÃ¶rÃ¼ntÃ¼leri.




--

## Tokenizasyon Nedir?

Tokenizasyon, modelin anlayabileceÄŸi **tamsayÄ±sal** temsillere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in ham metni daha kÃ¼Ã§Ã¼k parÃ§alara (tokenâ€™lara) ayÄ±rma iÅŸlemidir.  
Transformer tabanlÄ± modeller metni doÄŸrudan anlamaz; sayÄ± ile iÅŸlem yaparlar. Bu yÃ¼zden:

> â€œToken is an integer that represents a character, or a short segment of charactersâ€¦â€  
> â€”â€¯Tokenizer modÃ¼lÃ¼nÃ¼n tanÄ±mÄ±

Ã–rnek:
"Merhaba dÃ¼nya"â€¨â†’ ["Merhaba", "dÃ¼nya"]â€¨veyaâ€¨["Mer", "haba", "dÃ¼", "nya"]
---

## Tokenizasyon TÃ¼rleri

### Word-Level Tokenization (Kelime DÃ¼zeyi)

```python
def word_level_tokenize(text):
    return text.split()

print(word_level_tokenize("Merhaba dÃ¼nya, bugÃ¼n nasÄ±lsÄ±n?"))
```
# Ã‡Ä±ktÄ±: ['Merhaba', 'dÃ¼nya,', 'bugÃ¼n', 'nasÄ±lsÄ±n?']
Ne yapar? Metni boÅŸluk karakterinden bÃ¶lerek kelimeleri Ã§Ä±karÄ±r.
NasÄ±l Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r? Python 3 yÃ¼klÃ¼ bir ortamda bu fonksiyonu iÃ§eren dosyayÄ± Ã§alÄ±ÅŸtÄ±rabilirsiniz.
Character-Level Tokenization (Karakter DÃ¼zeyi)
```python
def character_level_tokenize(text):
    return list(text)

print(character_level_tokenize("ABC"))
```
# Ã‡Ä±ktÄ±: ['A', 'B', 'C']
Ne yapar? Her karakteri ayrÄ± bir token olarak iÅŸler.
Ne zaman tercih edilir? Dilin Ã§ok ince detaylarÄ±na girmek gerektiÄŸinde; ancak eÄŸitim maliyeti artar.
Subword Tokenization (Alt ParÃ§a DÃ¼zeyi)
Kelime ve karakter dÃ¼zeyleri arasÄ±nda bir denge saÄŸlar.
Ã–rnek:â€¨"bilinmeyen"
â†’ ["â–bil", "in", "me", "yen"]

AvantajÄ±: Nadir kelimelerde bile parÃ§alar tanÄ±nÄ±r; â€œunkâ€ token sayÄ±sÄ± azalÄ±r.

Subword Tokenizasyon AlgoritmalarÄ±
Byte-Pair Encoding (BPE)
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

text  = "bilinmeyen kelime"
tokens = tokenizer.tokenize(text)
ids    = tokenizer.convert_tokens_to_ids(tokens)

print("Tokens:", tokens)
print("IDs:", ids)
Ne yapar?
Karakterleri ayÄ±rÄ±r,
En sÄ±k gÃ¶rÃ¼len Ã§iftleri birleÅŸtirir,
Yeni tokenâ€™lar oluÅŸturur.
NasÄ±l kullanÄ±lÄ±r? transformers kÃ¼tÃ¼phanesini pip ile yÃ¼kleyip (pip install transformers) bu kodu Ã§alÄ±ÅŸtÄ±rÄ±n.
WordPiece
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text  = "bilinmeyen kelime"
tokens = tokenizer.tokenize(text)
ids    = tokenizer.convert_tokens_to_ids(tokens)

print("Tokens:", tokens)
print("IDs:", ids)
FarkÄ±: BPE yerine olasÄ±lÄ±ÄŸa dayalÄ± seÃ§im yapar.
AdÄ±mlar:
Karakterleri ayÄ±rÄ±r,
ParÃ§alarÄ±n birlikte gÃ¶rÃ¼lme olasÄ±lÄ±klarÄ±nÄ± hesaplar,
En yÃ¼ksek olasÄ±lÄ±klÄ± birleÅŸimleri gerÃ§ekleÅŸtirir.
SentencePiece
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/mt5-small")

text  = "bilinmeyen kelime"
tokens = tokenizer.tokenize(text)
ids    = tokenizer.convert_tokens_to_ids(tokens)

print("Tokens:", tokens)
print("IDs:", ids)
Ã–zellikleri:
Ã–n iÅŸlem yapmadan, bÃ¼tÃ¼n metni bir dize gibi ele alÄ±r.
BoÅŸluklarÄ± Ã¶zel karakter (_) ile temsil eder.
KullanÄ±m alanÄ±: Ã‡ok dilli veya morfolojik olarak zengin dillerde.

### Token IDâ€™leri ve Vocabulary Nedir?
Token â†’ ID dÃ¶nÃ¼ÅŸÃ¼mÃ¼: Model sayÄ±larla Ã§alÄ±ÅŸÄ±r.
Her modelin bir vocabulary dosyasÄ± (sÃ¶zlÃ¼ÄŸÃ¼) vardÄ±r.
Tokenâ€™lar eÄŸitim ve Ã§Ä±karÄ±m aÅŸamasÄ±nda bu IDâ€™ler Ã¼zerinden iÅŸlenir.

### ğŸ’¡ Tokenizer ve Detokenizer Nedir?
Tokenizer: Metni tokenâ€™lara bÃ¶lÃ¼p IDâ€™lere Ã§evirir.
Detokenizer: Model Ã§Ä±ktÄ±sÄ± IDâ€™leri tekrar metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
AkÄ±ÅŸ:
Metin
  â””â”€ Tokenizer â†’ [token1, token2, â€¦]
      â””â”€ Numeralizasyon â†’ [id1, id2, â€¦]
          â””â”€ Model Ã§alÄ±ÅŸÄ±r
              â””â”€ Detokenizer ile [id1, â€¦] â†’ [token1, â€¦]
                  â””â”€ Metni tekrar oluÅŸturur

### Neden Tokenizasyon Bu Kadar Ã–nemli?
Modelin kapasitesi, nadiren gÃ¶rÃ¼len kelimeleri tanÄ±ma ve genel baÅŸarÄ±mÄ± bÃ¼yÃ¼k Ã¶lÃ§Ã¼de buna baÄŸlÄ±dÄ±r.
TÃ¼rkÃ§e gibi eklemeli dillerde subword tokenizasyonu kritik avantaj saÄŸlar.





## yapay zeka eÄŸitebilmemiz iÃ§in etiketli verilere ihtiyaÃ§ duyarÄ±z (Supervised Learning)
Supervised Learning, yapay zekanÄ±n geÃ§miÅŸteki etiketli verilerden Ã¶ÄŸrenerek gelecekteki veriler iÃ§in doÄŸru tahminler yapabilmesini saÄŸlayan bir makine Ã¶ÄŸrenmesi yÃ¶ntemidir.




Rotary Positional Encoding'in (RoPE), daha eski konum kodlama yÃ¶ntemlerine gÃ¶re en temel avantajÄ± kelimelerin birbirlerine olan gÃ¶receli konumlarini ve mesafelerini de kodlayabilmesi.
RoPE fonksiyonunda, kelime vektÃ¶rlerini "dÃ¶ndÃ¼rmek" (rotate) iÃ§in sin cos fonksiyon Ã§ifti kullanÄ±lmÄ±ÅŸtÄ±r.



baÄŸlam iÃ§inde bulunduÄŸumuz yerde kurduÄŸumuz cÃ¼mledir veya iÃ§inde geÃ§tiÄŸi yer gÃ¶re kazandÄ±ÄŸÄ± anlam 






-----
n8n self hosted :
- starter kit  https://github.com/n8n-io/self-hosted-ai-starter-kit
- ollama -llm
Ã¶ÄŸren.

https://www.youtube.com/watch?v=huaCsAs02xY n8n hk. somut bir Ã¶rnek teÅŸkil ettiÄŸini dÃ¼ÅŸÃ¼nÃ¼yorum. 

----

n8n Starter Kit ve Ollama LLM entegrasyonu iÃ§in en kapsamlÄ± kaynaklar:

## n8n AI Starter Kit KaynaklarÄ±

**Resmi Kaynaklar:**
- **GitHub Repository:** n8n-io/self-hosted-ai-starter-kit - AÃ§Ä±k kaynak template
- **Resmi DokÃ¼mantasyon:** n8n.io/hosting/starter-kits/ai-starter-kit/
- **HÄ±zlÄ± BaÅŸlangÄ±Ã§:** n8n.io/try-it-out/quickstart/
- **AI Workflow Tutorial:** n8n.io/advanced-ai/intro-tutorial/
- **Learning Path:** n8n.io/learning-path/
- **Advanced AI Docs:** n8n.io/advanced-ai/

**Video ve Pratik Kaynaklar:**
- **Starter Kit Tutorial Video:** community.n8n.io/t/self-hosted-ai-starter-kit-tutorial/62465
- **Pratik Ã–rnek (Medium):** A practical n8n workflow example from A to Z

## Ollama + n8n Entegrasyon KaynaklarÄ±

**Resmi DokÃ¼mantasyon:**
- **Ollama Model Node:** n8n.io/integrations/builtin/cluster-nodes/.../lmollama/
- **Ollama Credentials:** n8n.io/integrations/builtin/credentials/ollama/
- **Common Issues:** Ollama Model node troubleshooting

**KapsamlÄ± Rehberler:**
- **Hostinger Tutorial:** n8n-ollama integration detaylÄ± rehber
- **Docker Guide:** n8n with Ollama LLM Locally Using Docker
- **Medium Tutorial:** n8n ve Ollama pipeline kurulumu
- **Advanced Integration:** Aleksandar Haber'in kapsamlÄ± rehberi

**HazÄ±r Workflow ÅablonlarÄ±:**
- **Chat Template:** Chat with local LLMs using n8n and Ollama
- **AI Agent Template:** Weather ve Wikipedia agent Ã¶rneÄŸi

## Ã–nerilen Ã–ÄŸrenme SÄ±rasÄ±:

1. **Temel n8n:** n8n quickstart ve learning path'i tamamlayÄ±n
2. **AI Starter Kit:** GitHub repo'yu klonlayÄ±p kurulum yapÄ±n
3. **Ollama Kurulum:** Yerel Ollama instance'Ä±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n
4. **Entegrasyon:** Docker rehberini takip ederek baÄŸlantÄ± kurun
5. **Pratik:** HazÄ±r workflow ÅŸablonlarÄ±nÄ± test edin
6. **GeliÅŸmiÅŸ:** Kendi AI agent'larÄ±nÄ±zÄ± geliÅŸtirin







```bash
brew install --cask docker
```

Kurulum bittikten sonra:

1. **Docker Desktop'u baÅŸlatÄ±n:**
```bash
open /Applications/Docker.app
```

2. **Docker'Ä±n Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± kontrol edin:**
```bash
docker --version
```

3. **ArdÄ±ndan n8n kurulumuna devam edin:**

```bash
# Ollama'yÄ± arka planda Ã§alÄ±ÅŸtÄ±r
ollama serve &

# .env dosyasÄ±nÄ± kontrol et
cat .env

# Docker Compose'u baÅŸlat
docker compose up
```
Kurulum bittikten sonra http://localhost:5678 adresinden n8n'e eriÅŸebileceksiniz.



Docker kurduktan sonra:

Ollama'yÄ± arka planda baÅŸlatÄ±n:

bashollama serve &

.env dosyasÄ±nÄ± kontrol edin:

bashcat .env
OLLAMA_HOST=host.docker.internal:11434 satÄ±rÄ±nÄ±n olduÄŸundan emin olun.

Docker Compose'u Ã§alÄ±ÅŸtÄ±rÄ±n:

bashdocker compose up

Llama3.2 kurun (opsiyonel ama Ã¶nerilen):

bashollama pull llama3.2








